{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831e97e1-1e47-46a9-8b7d-82c1cd7bcd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a GPU with the name: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        print(\"Found a GPU with the name:\", gpu)\n",
    "else:\n",
    "    print(\"Failed to detect a GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e779c1e-81f1-49dd-b078-6def9b6fddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d442ceac-7da8-4767-9e8e-5c223cb2d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = np.sort(glob.glob(\"./EEG FOLDER/*\"))\n",
    "# print(\"Total number of files: \", len(files))\n",
    "# print(\"Showing first 10 files...\")\n",
    "# files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7976214f-7ff8-4057-a158-1afa436bc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively glob for CSV files within each subdirectory of the EEG FOLDER\n",
    "files = glob.glob('./EEG FOLDER/*/*.csv', recursive=True)\n",
    "files.sort()  # Optional: sort the files for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e5e8ec-9394-48ea-ac23-8d7c801030d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  360\n",
      "Showing first 10 files...\n",
      "['./EEG FOLDER\\\\Fear\\\\cz_eeg_data_11.csv', './EEG FOLDER\\\\Fear\\\\cz_eeg_data_15.csv', './EEG FOLDER\\\\Fear\\\\cz_eeg_data_17.csv', './EEG FOLDER\\\\Fear\\\\cz_eeg_data_18.csv', './EEG FOLDER\\\\Fear\\\\cz_eeg_data_2.csv', './EEG FOLDER\\\\Fear\\\\cz_eeg_data_5.csv', './EEG FOLDER\\\\Fear\\\\ha_eeg_data_11.csv', './EEG FOLDER\\\\Fear\\\\ha_eeg_data_15.csv', './EEG FOLDER\\\\Fear\\\\ha_eeg_data_17.csv', './EEG FOLDER\\\\Fear\\\\ha_eeg_data_18.csv']\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of files: \", len(files))\n",
    "print(\"Showing first 10 files...\")\n",
    "print(files[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc943218-ce4b-4ee1-bc56-4ba941c7a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re            # To match regular expression for extracting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb25d5cd-7467-425e-8e94-2de5a4c04935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./EEG FOLDER\\\\Fear',\n",
       " './EEG FOLDER\\\\Happy',\n",
       " './EEG FOLDER\\\\Neutral',\n",
       " './EEG FOLDER\\\\Sad']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./EEG FOLDER/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953816cd-b945-46b8-9c15-7a1985b40d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./EEG FOLDER/Neutral\\\\cz_eeg_data_21.csv',\n",
       " './EEG FOLDER/Neutral\\\\cz_eeg_data_23.csv',\n",
       " './EEG FOLDER/Neutral\\\\cz_eeg_data_4.csv',\n",
       " './EEG FOLDER/Neutral\\\\cz_eeg_data_6.csv',\n",
       " './EEG FOLDER/Neutral\\\\cz_eeg_data_7.csv',\n",
       " './EEG FOLDER/Neutral\\\\cz_eeg_data_9.csv',\n",
       " './EEG FOLDER/Neutral\\\\ha_eeg_data_21.csv',\n",
       " './EEG FOLDER/Neutral\\\\ha_eeg_data_23.csv',\n",
       " './EEG FOLDER/Neutral\\\\ha_eeg_data_4.csv',\n",
       " './EEG FOLDER/Neutral\\\\ha_eeg_data_6.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./EEG FOLDER/Neutral/*\")[:10] # Showing first 10 files of Neutral folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddbd2d6a-0990-44fa-a81b-1b8059d6a857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./EEG FOLDER/Sad\\\\cz_eeg_data_1.csv',\n",
       " './EEG FOLDER/Sad\\\\cz_eeg_data_10.csv',\n",
       " './EEG FOLDER/Sad\\\\cz_eeg_data_12.csv',\n",
       " './EEG FOLDER/Sad\\\\cz_eeg_data_13.csv',\n",
       " './EEG FOLDER/Sad\\\\cz_eeg_data_14.csv',\n",
       " './EEG FOLDER/Sad\\\\cz_eeg_data_8.csv',\n",
       " './EEG FOLDER/Sad\\\\ha_eeg_data_1.csv',\n",
       " './EEG FOLDER/Sad\\\\ha_eeg_data_10.csv',\n",
       " './EEG FOLDER/Sad\\\\ha_eeg_data_12.csv',\n",
       " './EEG FOLDER/Sad\\\\ha_eeg_data_13.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./EEG FOLDER/Sad/*\")[:10]  #Sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7054c0f0-c013-4ad7-833b-fe3682ba398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./EEG FOLDER/Fear\\\\cz_eeg_data_11.csv',\n",
       " './EEG FOLDER/Fear\\\\cz_eeg_data_15.csv',\n",
       " './EEG FOLDER/Fear\\\\cz_eeg_data_17.csv',\n",
       " './EEG FOLDER/Fear\\\\cz_eeg_data_18.csv',\n",
       " './EEG FOLDER/Fear\\\\cz_eeg_data_2.csv',\n",
       " './EEG FOLDER/Fear\\\\cz_eeg_data_5.csv',\n",
       " './EEG FOLDER/Fear\\\\ha_eeg_data_11.csv',\n",
       " './EEG FOLDER/Fear\\\\ha_eeg_data_15.csv',\n",
       " './EEG FOLDER/Fear\\\\ha_eeg_data_17.csv',\n",
       " './EEG FOLDER/Fear\\\\ha_eeg_data_18.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./EEG FOLDER/Fear/*\")[:10]  #Fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d503607-0b83-4a6c-a814-3ef3b25c0fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./EEG FOLDER/Happy\\\\cz_eeg_data_16.csv',\n",
       " './EEG FOLDER/Happy\\\\cz_eeg_data_19.csv',\n",
       " './EEG FOLDER/Happy\\\\cz_eeg_data_20.csv',\n",
       " './EEG FOLDER/Happy\\\\cz_eeg_data_22.csv',\n",
       " './EEG FOLDER/Happy\\\\cz_eeg_data_24.csv',\n",
       " './EEG FOLDER/Happy\\\\cz_eeg_data_3.csv',\n",
       " './EEG FOLDER/Happy\\\\ha_eeg_data_16.csv',\n",
       " './EEG FOLDER/Happy\\\\ha_eeg_data_19.csv',\n",
       " './EEG FOLDER/Happy\\\\ha_eeg_data_20.csv',\n",
       " './EEG FOLDER/Happy\\\\ha_eeg_data_22.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./EEG FOLDER/Happy/*\")[:10]  #Happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffc3eb9d-a616-48a1-9abe-aaeece88909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(file_list, batch_size=20):\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i * batch_size >= len(file_list):  # This loop is used to run the generator indefinitely.\n",
    "            i = 0\n",
    "            np.random.shuffle(file_list)\n",
    "        else:\n",
    "            file_chunk = file_list[i * batch_size:(i + 1) * batch_size]\n",
    "            data = []\n",
    "            labels = []\n",
    "            label_classes = [\"Neutral\", \"Sad\", \"Fear\", \"Happy\"]\n",
    "            for file_path in file_chunk:\n",
    "                directory_path = os.path.dirname(file_path)\n",
    "                label = os.path.basename(directory_path)  # This should correctly extract the folder name as the label\n",
    "                label_index = label_classes.index(label)  # Get the index of the label in label_classes\n",
    "                temp = pd.read_csv(file_path)\n",
    "                temp = temp.iloc[:, 1:].values.reshape(400, 62, 1)\n",
    "                data.append(temp)\n",
    "                labels.append(label_index)\n",
    "            data = np.asarray(data).reshape(-1, 400, 62, 1)\n",
    "            labels = np.asarray(labels)\n",
    "            yield data, labels\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e73635d-177b-408c-9f27-d3adc60754d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = data_generator(files, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457cda4f-cf8c-4687-a467-ca7ced4c3b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a38a3dc-04ef-46d1-ae45-7c2138c6420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 400, 62, 1) (100,)\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3] <--Labels\n",
      "\n",
      "(100, 400, 62, 1) (100,)\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] <--Labels\n",
      "\n",
      "(100, 400, 62, 1) (100,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] <--Labels\n",
      "\n",
      "(60, 400, 62, 1) (60,)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] <--Labels\n",
      "\n",
      "(100, 400, 62, 1) (100,)\n",
      "[1 3 1 0 2 2 1 2 0 3 0 3 2 0 0 0 1 1 1 3 1 0 3 3 0 1 3 3 1 2 2 2 1 2 2 2 2\n",
      " 0 1 2 3 1 2 3 1 0 3 3 2 3 0 0 0 3 3 3 2 0 1 2 3 0 2 2 0 2 0 1 3 1 0 1 2 3\n",
      " 2 0 2 0 0 1 1 3 2 2 0 0 2 3 2 3 3 2 3 2 0 0 3 1 3 2] <--Labels\n",
      "\n",
      "(100, 400, 62, 1) (100,)\n",
      "[3 3 1 1 0 1 0 1 1 0 2 3 2 2 2 0 1 1 0 3 0 2 1 2 1 0 3 3 3 3 2 0 2 0 2 1 0\n",
      " 2 3 2 1 0 3 1 0 1 2 1 0 1 2 3 0 3 3 3 1 1 3 2 1 3 1 2 2 1 0 0 1 1 3 0 0 3\n",
      " 3 2 1 1 1 1 1 0 3 3 2 1 3 2 3 0 1 1 3 0 0 3 0 3 0 0] <--Labels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for data, labels in generated_data:\n",
    "    print(data.shape, labels.shape)\n",
    "    print(labels, \"<--Labels\")  # Just to see the lables\n",
    "    print()\n",
    "    num = num + 1\n",
    "    if num > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76202134-5d57-4852-bcf0-e31c3e0b7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48886f72-80ee-4ce8-b9f3-aaae991b9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_data_generator(file_list, batch_size = 20):\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i*batch_size >= len(file_list):  \n",
    "            i = 0\n",
    "            np.random.shuffle(file_list)\n",
    "        else:\n",
    "            file_chunk = file_list[i*batch_size:(i+1)*batch_size] \n",
    "            data = []\n",
    "            labels = []\n",
    "            label_classes = tf.constant([\"Neutral\", \"Sad\", \"Fear\", \"Happy\"]) # This line has changed.\n",
    "            for file in file_chunk:\n",
    "                directory_path = os.path.dirname(file)\n",
    "                label = os.path.basename(directory_path)  # This should correctly extract the folder name as the label\n",
    "                label_index = tf.where(tf.equal(label_classes, label))  # Find index of label in label_classes\n",
    "                label_index = tf.squeeze(label_index)  # Remove extra dimensions\n",
    "                temp = pd.read_csv(open(file,'r'))\n",
    "                temp = temp.iloc[:, 1:].values  # Convert DataFrame to numpy array after dropping the first column\n",
    "\n",
    "                # Normalize each feature to zero mean and unit variance\n",
    "                mean = np.mean(temp, axis=0)\n",
    "                std = np.std(temp, axis=0)\n",
    "                normalized_temp = (temp - mean) / (std + 1e-8)  # Adding epsilon to avoid division by zero\n",
    "\n",
    "                # Reshape for the model\n",
    "                normalized_temp = normalized_temp.reshape(400, 62, 1)\n",
    "                data.append(normalized_temp)\n",
    "           \n",
    "                labels.append(label_index)\n",
    "\n",
    "            data = np.asarray(data).reshape(-1,400,62,1)\n",
    "            labels = np.asarray(labels)\n",
    "            yield data, labels\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69f3764c-7b23-42fe-9ce2-8033b3efb6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data = tf_data_generator(files, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "adf0115a-16c3-44f6-b4aa-286171196897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 400, 62, 1) (10,)\n",
      "[2 0 1 3 1 2 1 1 1 3] <--Labels\n",
      "\n",
      "(10, 400, 62, 1) (10,)\n",
      "[2 2 0 3 1 2 1 2 2 2] <--Labels\n",
      "\n",
      "(10, 400, 62, 1) (10,)\n",
      "[0 0 1 2 0 0 1 0 3 3] <--Labels\n",
      "\n",
      "(10, 400, 62, 1) (10,)\n",
      "[2 3 3 0 2 0 0 1 2 1] <--Labels\n",
      "\n",
      "(10, 400, 62, 1) (10,)\n",
      "[1 0 0 1 2 1 0 1 1 1] <--Labels\n",
      "\n",
      "(10, 400, 62, 1) (10,)\n",
      "[2 3 2 0 1 2 0 0 2 2] <--Labels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for data, labels in check_data:\n",
    "    print(data.shape, labels.shape)\n",
    "    print(labels, \"<--Labels\")\n",
    "    print()\n",
    "    num = num + 1\n",
    "    if num > 5: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e2de71ca-839a-44a0-b6c9-15e445129640",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "dataset = tf.data.Dataset.from_generator(tf_data_generator,args= [files, batch_size],output_types = (tf.float32, tf.float32),\n",
    "                                                output_shapes = ((None,400,62,1),(None,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "98b5840a-0083-413e-8dac-5ae12b4e5628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 400, 62, 1) (15,)\n",
      "tf.Tensor([2. 0. 1. 3. 1. 2. 1. 1. 1. 3. 2. 2. 0. 3. 1.], shape=(15,), dtype=float32)\n",
      "\n",
      "(15, 400, 62, 1) (15,)\n",
      "tf.Tensor([2. 1. 2. 2. 2. 0. 0. 1. 2. 0. 0. 1. 0. 3. 3.], shape=(15,), dtype=float32)\n",
      "\n",
      "(15, 400, 62, 1) (15,)\n",
      "tf.Tensor([2. 3. 3. 0. 2. 0. 0. 1. 2. 1. 1. 0. 0. 1. 2.], shape=(15,), dtype=float32)\n",
      "\n",
      "(15, 400, 62, 1) (15,)\n",
      "tf.Tensor([1. 0. 1. 1. 1. 2. 3. 2. 0. 1. 2. 0. 0. 2. 2.], shape=(15,), dtype=float32)\n",
      "\n",
      "(15, 400, 62, 1) (15,)\n",
      "tf.Tensor([3. 1. 2. 2. 1. 3. 2. 2. 3. 1. 0. 0. 2. 2. 1.], shape=(15,), dtype=float32)\n",
      "\n",
      "(15, 400, 62, 1) (15,)\n",
      "tf.Tensor([3. 0. 3. 3. 0. 1. 3. 0. 3. 2. 2. 2. 2. 0. 0.], shape=(15,), dtype=float32)\n",
      "\n",
      "(15, 400, 62, 1) (15,)\n",
      "tf.Tensor([0. 0. 1. 2. 1. 2. 2. 0. 0. 1. 0. 0. 3. 3. 3.], shape=(15,), dtype=float32)\n",
      "\n",
      "(15, 400, 62, 1) (15,)\n",
      "tf.Tensor([1. 3. 1. 3. 0. 3. 3. 3. 0. 1. 1. 0. 2. 1. 2.], shape=(15,), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check whether dataset works or not.\n",
    "num = 0\n",
    "for data, labels in dataset:\n",
    "    print(data.shape, labels.shape)\n",
    "    print(labels)\n",
    "    print()\n",
    "    num = num + 1\n",
    "    if num > 7: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "105ac942-89ee-459e-b787-f1983b93bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building data pipeline and training CNN modelÂ¶\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2a2d5-d22c-4496-bde1-0a2d6d24f1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7be4c349-0fac-47bd-a9de-9f36590defe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Neutral_files = glob.glob(\"./EEG FOLDER/Neutral/*\")\n",
    "Sad_files = glob.glob(\"./EEG FOLDER/Sad/*\")\n",
    "Fear_files = glob.glob(\"./EEG FOLDER/Fear/*\")\n",
    "Happy_files = glob.glob(\"./EEG FOLDER/Happy/*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b181c05e-6a95-4be6-9dfc-a3e0ef56c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d2f84d8e-ef55-487f-88ef-25df244da48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Neutral_train, Neutral_test = train_test_split(Neutral_files, test_size = .20, random_state = 5)\n",
    "Sad_train, Sad_test = train_test_split(Sad_files, test_size = .20, random_state = 54)\n",
    "Fear_train, Fear_test = train_test_split(Fear_files, test_size = .20, random_state = 543)\n",
    "Happy_train, Happy_test = train_test_split(Happy_files, test_size = .20, random_state = 5432)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a26ca310-c8b8-400c-906b-175ddb5564f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Neutral_train, Neutral_val = train_test_split(Neutral_train, test_size = .10, random_state = 1)\n",
    "Sad_train, Sad_val = train_test_split(Sad_train, test_size = .10, random_state = 12)\n",
    "Fear_train, Fear_val = train_test_split(Fear_train, test_size = .10, random_state = 123)\n",
    "Happy_train, Happy_val = train_test_split(Happy_train, test_size = .10, random_state = 1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "86759485-b300-4130-93f9-c9f9f9c12cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_names = Neutral_train + Sad_train + Fear_train + Happy_train \n",
    "validation_file_names = Neutral_val + Sad_val + Fear_val + Happy_val\n",
    "test_file_names = Neutral_test + Sad_test + Fear_test + Happy_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2c7c619b-3e1e-4770-81d0-484e0d36fff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train_files: 256\n",
      "Number of validation_files: 32\n",
      "Number of test_files: 72\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train_files:\" ,len(train_file_names))\n",
    "print(\"Number of validation_files:\" ,len(validation_file_names))\n",
    "print(\"Number of test_files:\" ,len(test_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "37885d64-1be7-4da5-b265-ca8acf95d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 18\n",
    "train_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [train_file_names, batch_size], \n",
    "                                              output_shapes = ((None,400,62,1),(None,)),\n",
    "                                              output_types = (tf.float32, tf.float32))\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [validation_file_names, batch_size],\n",
    "                                                   output_shapes = ((None,400,62,1),(None,)),\n",
    "                                                   output_types = (tf.float32, tf.float32))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [test_file_names, batch_size],\n",
    "                                             output_shapes = ((None,400,62,1),(None,)),\n",
    "                                             output_types = (tf.float32, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d13e9b3f-d44b-40d9-905f-fda0ae258019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the model.\n",
    "from tensorflow.keras import layers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "04445e96-6ee5-49f3-9b67-0dc696ae45bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_74 (Conv2D)          (None, 398, 60, 32)       320       \n",
      "                                                                 \n",
      " max_pooling2d_68 (MaxPoolin  (None, 199, 30, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_75 (Conv2D)          (None, 197, 28, 64)       18496     \n",
      "                                                                 \n",
      " max_pooling2d_69 (MaxPoolin  (None, 98, 14, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_76 (Conv2D)          (None, 96, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_70 (MaxPoolin  (None, 48, 6, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_77 (Conv2D)          (None, 46, 4, 256)        295168    \n",
      "                                                                 \n",
      " max_pooling2d_71 (MaxPoolin  (None, 23, 2, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_19 (Flatten)        (None, 11776)             0         \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 128)               1507456   \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,903,877\n",
      "Trainable params: 1,903,877\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     layers.Conv2D(16, 3, activation = \"relu\", input_shape = (400,62,1)),\n",
    "#     layers.MaxPool2D(2),\n",
    "#     layers.Conv2D(62, 3, activation = \"relu\"),\n",
    "#     layers.MaxPool2D(2),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(16, activation = \"relu\"),\n",
    "#     layers.Dense(5, activation = \"softmax\")\n",
    "# ])\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, 3, activation=\"relu\", input_shape=(400, 62, 1)),\n",
    "    layers.MaxPool2D(2),\n",
    "    layers.Conv2D(64, 3, activation=\"relu\"),\n",
    "    layers.MaxPool2D(2),\n",
    "    layers.Conv2D(128, 3, activation=\"relu\"),\n",
    "    layers.MaxPool2D(2),\n",
    "    layers.Conv2D(256, 3, activation=\"relu\"),\n",
    "    layers.MaxPool2D(2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(5, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=5,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore weights from the epoch with the best validation loss\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "31c7fd16-4150-4d42-889b-c642a6b07414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model.\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9ae9fd13-3dee-4242-8936-13407fac868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_per_epoch =  15\n",
      "validation_steps =  2\n",
      "steps =  4\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = int(np.ceil(len(train_file_names)/batch_size))\n",
    "validation_steps = int(np.ceil(len(validation_file_names)/batch_size))\n",
    "steps = int(np.ceil(len(test_file_names)/batch_size))\n",
    "print(\"steps_per_epoch = \", steps_per_epoch)\n",
    "print(\"validation_steps = \", validation_steps)\n",
    "print(\"steps = \", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6b892b76-ab77-4e67-8b87-59d8f5343a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 12s 580ms/step - loss: 2.3882 - accuracy: 0.2070 - val_loss: 1.6005 - val_accuracy: 0.2500\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 7s 505ms/step - loss: 1.5764 - accuracy: 0.2188 - val_loss: 1.4740 - val_accuracy: 0.2500\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 8s 524ms/step - loss: 1.5145 - accuracy: 0.2031 - val_loss: 1.4940 - val_accuracy: 0.2500\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 8s 545ms/step - loss: 1.4747 - accuracy: 0.2617 - val_loss: 1.4166 - val_accuracy: 0.2500\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 8s 542ms/step - loss: 1.4780 - accuracy: 0.2461 - val_loss: 1.4395 - val_accuracy: 0.2500\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 8s 536ms/step - loss: 1.4698 - accuracy: 0.2773 - val_loss: 1.4449 - val_accuracy: 0.2500\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 8s 513ms/step - loss: 1.4464 - accuracy: 0.2734 - val_loss: 1.4135 - val_accuracy: 0.2500\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 8s 505ms/step - loss: 1.4482 - accuracy: 0.2344 - val_loss: 1.4185 - val_accuracy: 0.2500\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 8s 518ms/step - loss: 1.4403 - accuracy: 0.2891 - val_loss: 1.4122 - val_accuracy: 0.2500\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 8s 524ms/step - loss: 1.4612 - accuracy: 0.2461 - val_loss: 1.4204 - val_accuracy: 0.2500\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 7s 500ms/step - loss: 1.4503 - accuracy: 0.2734 - val_loss: 1.4087 - val_accuracy: 0.2500\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 8s 512ms/step - loss: 1.4442 - accuracy: 0.2617 - val_loss: 1.4023 - val_accuracy: 0.2500\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 8s 527ms/step - loss: 1.4392 - accuracy: 0.2109 - val_loss: 1.4059 - val_accuracy: 0.2500\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 8s 510ms/step - loss: 1.4336 - accuracy: 0.2422 - val_loss: 1.4066 - val_accuracy: 0.2500\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 8s 505ms/step - loss: 1.4443 - accuracy: 0.1836 - val_loss: 1.4132 - val_accuracy: 0.2500\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 8s 529ms/step - loss: 1.4219 - accuracy: 0.2852 - val_loss: 1.3939 - val_accuracy: 0.2500\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 8s 524ms/step - loss: 1.4170 - accuracy: 0.2305 - val_loss: 1.4003 - val_accuracy: 0.2500\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 8s 522ms/step - loss: 1.4328 - accuracy: 0.2188 - val_loss: 1.4039 - val_accuracy: 0.2500\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 8s 528ms/step - loss: 1.4160 - accuracy: 0.2500 - val_loss: 1.3968 - val_accuracy: 0.2500\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 7s 504ms/step - loss: 1.4233 - accuracy: 0.2500 - val_loss: 1.3952 - val_accuracy: 0.2500\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 8s 539ms/step - loss: 1.4041 - accuracy: 0.2891 - val_loss: 1.3914 - val_accuracy: 0.2500\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 8s 533ms/step - loss: 1.4256 - accuracy: 0.2500 - val_loss: 1.4018 - val_accuracy: 0.2500\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 8s 515ms/step - loss: 1.4207 - accuracy: 0.2344 - val_loss: 1.3995 - val_accuracy: 0.2500\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 7s 493ms/step - loss: 1.4200 - accuracy: 0.2852 - val_loss: 1.3985 - val_accuracy: 0.2500\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 7s 493ms/step - loss: 1.4254 - accuracy: 0.2617 - val_loss: 1.3998 - val_accuracy: 0.2500\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 8s 507ms/step - loss: 1.4067 - accuracy: 0.2695 - val_loss: 1.3940 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28b3c7e8460>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,\n",
    "         validation_steps = validation_steps, epochs = 100, callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c6d835cb-c50c-4fed-a627-1da2513894a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 5s 490ms/step - loss: 1.3936 - accuracy: 0.2278\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset, steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b1253738-fa3f-4835-95d4-2a9e3d84a2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  1.726000189781189\n",
      "Test accuracy: 0.1875\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: \", test_loss)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb7c10-1c85-40b9-8dc0-9fa3cb789daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
