{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831e97e1-1e47-46a9-8b7d-82c1cd7bcd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a GPU with the name: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        print(\"Found a GPU with the name:\", gpu)\n",
    "else:\n",
    "    print(\"Failed to detect a GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e779c1e-81f1-49dd-b078-6def9b6fddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d442ceac-7da8-4767-9e8e-5c223cb2d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = np.sort(glob.glob(\"./EEG FOLDER/*\"))\n",
    "# print(\"Total number of files: \", len(files))\n",
    "# print(\"Showing first 10 files...\")\n",
    "# files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7976214f-7ff8-4057-a158-1afa436bc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively glob for CSV files within each subdirectory of the EEG FOLDER\n",
    "files = glob.glob('./EEG FOLDER/*/*.csv', recursive=True)\n",
    "files.sort()  # Optional: sort the files for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e5e8ec-9394-48ea-ac23-8d7c801030d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  360\n",
      "Showing first 10 files...\n",
      "['./EEG FOLDER\\\\Fear\\\\cz_eeg_data_11.csv', './EEG FOLDER\\\\Fear\\\\cz_eeg_data_15.csv', './EEG FOLDER\\\\Fear\\\\cz_eeg_data_17.csv', './EEG FOLDER\\\\Fear\\\\cz_eeg_data_18.csv', './EEG FOLDER\\\\Fear\\\\cz_eeg_data_2.csv', './EEG FOLDER\\\\Fear\\\\cz_eeg_data_5.csv', './EEG FOLDER\\\\Fear\\\\ha_eeg_data_11.csv', './EEG FOLDER\\\\Fear\\\\ha_eeg_data_15.csv', './EEG FOLDER\\\\Fear\\\\ha_eeg_data_17.csv', './EEG FOLDER\\\\Fear\\\\ha_eeg_data_18.csv']\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of files: \", len(files))\n",
    "print(\"Showing first 10 files...\")\n",
    "print(files[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc943218-ce4b-4ee1-bc56-4ba941c7a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re            # To match regular expression for extracting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb25d5cd-7467-425e-8e94-2de5a4c04935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./EEG FOLDER\\\\Fear',\n",
       " './EEG FOLDER\\\\Happy',\n",
       " './EEG FOLDER\\\\Neutral',\n",
       " './EEG FOLDER\\\\Sad']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./EEG FOLDER/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953816cd-b945-46b8-9c15-7a1985b40d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./EEG FOLDER/Neutral\\\\cz_eeg_data_21.csv',\n",
       " './EEG FOLDER/Neutral\\\\cz_eeg_data_23.csv',\n",
       " './EEG FOLDER/Neutral\\\\cz_eeg_data_4.csv',\n",
       " './EEG FOLDER/Neutral\\\\cz_eeg_data_6.csv',\n",
       " './EEG FOLDER/Neutral\\\\cz_eeg_data_7.csv',\n",
       " './EEG FOLDER/Neutral\\\\cz_eeg_data_9.csv',\n",
       " './EEG FOLDER/Neutral\\\\ha_eeg_data_21.csv',\n",
       " './EEG FOLDER/Neutral\\\\ha_eeg_data_23.csv',\n",
       " './EEG FOLDER/Neutral\\\\ha_eeg_data_4.csv',\n",
       " './EEG FOLDER/Neutral\\\\ha_eeg_data_6.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./EEG FOLDER/Neutral/*\")[:10] # Showing first 10 files of Neutral folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddbd2d6a-0990-44fa-a81b-1b8059d6a857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./EEG FOLDER/Sad\\\\cz_eeg_data_1.csv',\n",
       " './EEG FOLDER/Sad\\\\cz_eeg_data_10.csv',\n",
       " './EEG FOLDER/Sad\\\\cz_eeg_data_12.csv',\n",
       " './EEG FOLDER/Sad\\\\cz_eeg_data_13.csv',\n",
       " './EEG FOLDER/Sad\\\\cz_eeg_data_14.csv',\n",
       " './EEG FOLDER/Sad\\\\cz_eeg_data_8.csv',\n",
       " './EEG FOLDER/Sad\\\\ha_eeg_data_1.csv',\n",
       " './EEG FOLDER/Sad\\\\ha_eeg_data_10.csv',\n",
       " './EEG FOLDER/Sad\\\\ha_eeg_data_12.csv',\n",
       " './EEG FOLDER/Sad\\\\ha_eeg_data_13.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./EEG FOLDER/Sad/*\")[:10]  #Sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7054c0f0-c013-4ad7-833b-fe3682ba398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./EEG FOLDER/Fear\\\\cz_eeg_data_11.csv',\n",
       " './EEG FOLDER/Fear\\\\cz_eeg_data_15.csv',\n",
       " './EEG FOLDER/Fear\\\\cz_eeg_data_17.csv',\n",
       " './EEG FOLDER/Fear\\\\cz_eeg_data_18.csv',\n",
       " './EEG FOLDER/Fear\\\\cz_eeg_data_2.csv',\n",
       " './EEG FOLDER/Fear\\\\cz_eeg_data_5.csv',\n",
       " './EEG FOLDER/Fear\\\\ha_eeg_data_11.csv',\n",
       " './EEG FOLDER/Fear\\\\ha_eeg_data_15.csv',\n",
       " './EEG FOLDER/Fear\\\\ha_eeg_data_17.csv',\n",
       " './EEG FOLDER/Fear\\\\ha_eeg_data_18.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./EEG FOLDER/Fear/*\")[:10]  #Fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d503607-0b83-4a6c-a814-3ef3b25c0fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./EEG FOLDER/Happy\\\\cz_eeg_data_16.csv',\n",
       " './EEG FOLDER/Happy\\\\cz_eeg_data_19.csv',\n",
       " './EEG FOLDER/Happy\\\\cz_eeg_data_20.csv',\n",
       " './EEG FOLDER/Happy\\\\cz_eeg_data_22.csv',\n",
       " './EEG FOLDER/Happy\\\\cz_eeg_data_24.csv',\n",
       " './EEG FOLDER/Happy\\\\cz_eeg_data_3.csv',\n",
       " './EEG FOLDER/Happy\\\\ha_eeg_data_16.csv',\n",
       " './EEG FOLDER/Happy\\\\ha_eeg_data_19.csv',\n",
       " './EEG FOLDER/Happy\\\\ha_eeg_data_20.csv',\n",
       " './EEG FOLDER/Happy\\\\ha_eeg_data_22.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./EEG FOLDER/Happy/*\")[:10]  #Happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffc3eb9d-a616-48a1-9abe-aaeece88909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(file_list, batch_size=20):\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i * batch_size >= len(file_list):  # This loop is used to run the generator indefinitely.\n",
    "            i = 0\n",
    "            np.random.shuffle(file_list)\n",
    "        else:\n",
    "            file_chunk = file_list[i * batch_size:(i + 1) * batch_size]\n",
    "            data = []\n",
    "            labels = []\n",
    "            label_classes = [\"Neutral\", \"Sad\", \"Fear\", \"Happy\"]\n",
    "            for file_path in file_chunk:\n",
    "                directory_path = os.path.dirname(file_path)\n",
    "                label = os.path.basename(directory_path)  # This should correctly extract the folder name as the label\n",
    "                label_index = label_classes.index(label)  # Get the index of the label in label_classes\n",
    "                temp = pd.read_csv(file_path)\n",
    "                temp = temp.iloc[:, 1:].values.reshape(400, 62, 1)\n",
    "                data.append(temp)\n",
    "                labels.append(label_index)\n",
    "            data = np.asarray(data).reshape(-1, 400, 62, 1)\n",
    "            labels = np.asarray(labels)\n",
    "            yield data, labels\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e73635d-177b-408c-9f27-d3adc60754d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = data_generator(files, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457cda4f-cf8c-4687-a467-ca7ced4c3b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a38a3dc-04ef-46d1-ae45-7c2138c6420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 400, 62, 1) (100,)\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3] <--Labels\n",
      "\n",
      "(100, 400, 62, 1) (100,)\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] <--Labels\n",
      "\n",
      "(100, 400, 62, 1) (100,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] <--Labels\n",
      "\n",
      "(60, 400, 62, 1) (60,)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] <--Labels\n",
      "\n",
      "(100, 400, 62, 1) (100,)\n",
      "[1 3 1 0 2 2 1 2 0 3 0 3 2 0 0 0 1 1 1 3 1 0 3 3 0 1 3 3 1 2 2 2 1 2 2 2 2\n",
      " 0 1 2 3 1 2 3 1 0 3 3 2 3 0 0 0 3 3 3 2 0 1 2 3 0 2 2 0 2 0 1 3 1 0 1 2 3\n",
      " 2 0 2 0 0 1 1 3 2 2 0 0 2 3 2 3 3 2 3 2 0 0 3 1 3 2] <--Labels\n",
      "\n",
      "(100, 400, 62, 1) (100,)\n",
      "[3 3 1 1 0 1 0 1 1 0 2 3 2 2 2 0 1 1 0 3 0 2 1 2 1 0 3 3 3 3 2 0 2 0 2 1 0\n",
      " 2 3 2 1 0 3 1 0 1 2 1 0 1 2 3 0 3 3 3 1 1 3 2 1 3 1 2 2 1 0 0 1 1 3 0 0 3\n",
      " 3 2 1 1 1 1 1 0 3 3 2 1 3 2 3 0 1 1 3 0 0 3 0 3 0 0] <--Labels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for data, labels in generated_data:\n",
    "    print(data.shape, labels.shape)\n",
    "    print(labels, \"<--Labels\")  # Just to see the lables\n",
    "    print()\n",
    "    num = num + 1\n",
    "    if num > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76202134-5d57-4852-bcf0-e31c3e0b7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48886f72-80ee-4ce8-b9f3-aaae991b9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_data_generator(file_list, batch_size = 20):\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i*batch_size >= len(file_list):  \n",
    "            i = 0\n",
    "            np.random.shuffle(file_list)\n",
    "        else:\n",
    "            file_chunk = file_list[i*batch_size:(i+1)*batch_size] \n",
    "            data = []\n",
    "            labels = []\n",
    "            label_classes = tf.constant([\"Neutral\", \"Sad\", \"Fear\", \"Happy\"]) # This line has changed.\n",
    "            for file in file_chunk:\n",
    "                directory_path = os.path.dirname(file)\n",
    "                label = os.path.basename(directory_path)  # This should correctly extract the folder name as the label\n",
    "                label_index = tf.where(tf.equal(label_classes, label))  # Find index of label in label_classes\n",
    "                label_index = tf.squeeze(label_index)  # Remove extra dimensions\n",
    "                temp = pd.read_csv(open(file,'r'))\n",
    "                temp = temp.iloc[:, 1:].values  # Convert DataFrame to numpy array after dropping the first column\n",
    "\n",
    "                # Normalize each feature to zero mean and unit variance\n",
    "                mean = np.mean(temp, axis=0)\n",
    "                std = np.std(temp, axis=0)\n",
    "                normalized_temp = (temp - mean) / (std + 1e-8)  # Adding epsilon to avoid division by zero\n",
    "\n",
    "                # Reshape for the model\n",
    "                normalized_temp = normalized_temp.reshape(400, 62, 1)\n",
    "                data.append(normalized_temp)\n",
    "           \n",
    "                labels.append(label_index)\n",
    "\n",
    "            data = np.asarray(data).reshape(-1,400,62,1)\n",
    "            labels = np.asarray(labels)\n",
    "            yield data, labels\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69f3764c-7b23-42fe-9ce2-8033b3efb6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data = tf_data_generator(files, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adf0115a-16c3-44f6-b4aa-286171196897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 400, 62, 1) (100,)\n",
      "[3 0 2 1 0 0 3 2 3 1 1 1 3 1 1 3 0 3 1 1 2 2 3 2 3 0 1 2 3 1 0 0 1 3 0 2 0\n",
      " 1 0 2 2 2 1 2 3 3 3 2 1 0 0 1 2 3 2 2 2 3 3 3 2 2 3 1 0 0 1 3 1 0 0 3 1 3\n",
      " 1 0 0 0 0 0 2 0 1 0 3 0 2 1 3 0 3 0 0 0 3 3 1 3 1 1] <--Labels\n",
      "\n",
      "(100, 400, 62, 1) (100,)\n",
      "[0 2 0 1 2 1 3 1 2 2 2 3 3 1 1 2 2 3 2 2 1 0 3 3 3 1 3 1 0 3 1 2 0 2 1 1 0\n",
      " 0 1 2 1 1 2 0 3 1 3 1 0 3 3 1 3 3 3 1 2 3 0 0 2 3 3 1 1 0 2 0 3 2 2 3 1 0\n",
      " 2 1 3 2 2 2 2 2 0 0 3 1 0 3 0 3 3 2 1 2 3 3 3 2 1 0] <--Labels\n",
      "\n",
      "(100, 400, 62, 1) (100,)\n",
      "[3 0 1 3 2 1 3 0 3 2 2 0 2 3 2 2 1 2 0 0 1 3 1 0 3 0 0 0 1 0 0 0 1 1 1 1 0\n",
      " 2 0 2 1 3 2 0 2 1 2 2 1 0 1 2 2 0 1 0 1 0 3 3 3 3 3 0 3 1 3 1 0 1 2 0 0 2\n",
      " 1 2 0 2 3 1 3 1 1 0 1 3 3 2 3 1 2 0 2 0 3 2 3 0 2 1] <--Labels\n",
      "\n",
      "(60, 400, 62, 1) (60,)\n",
      "[3 2 3 2 3 3 1 1 2 2 2 2 3 0 0 2 2 1 0 3 1 1 0 0 2 3 2 3 1 1 0 0 0 0 1 3 3\n",
      " 2 0 2 1 1 1 3 2 2 0 2 1 1 0 2 3 2 1 2 2 0 0 0] <--Labels\n",
      "\n",
      "(100, 400, 62, 1) (100,)\n",
      "[2 0 1 3 1 2 1 1 1 3 2 2 0 3 1 2 1 2 2 2 0 0 1 2 0 0 1 0 3 3 2 3 3 0 2 0 0\n",
      " 1 2 1 1 0 0 1 2 1 0 1 1 1 2 3 2 0 1 2 0 0 2 2 3 1 2 2 1 3 2 2 3 1 0 0 2 2\n",
      " 1 3 0 3 3 0 1 3 0 3 2 2 2 2 0 0 0 0 1 2 1 2 2 0 0 1] <--Labels\n",
      "\n",
      "(100, 400, 62, 1) (100,)\n",
      "[0 0 3 3 3 1 3 1 3 0 3 3 3 0 1 1 0 2 1 2 2 3 3 1 3 3 1 3 2 1 3 0 2 2 2 2 1\n",
      " 1 3 1 2 3 0 1 2 0 1 1 1 2 1 2 1 3 3 3 3 3 3 3 2 2 2 1 1 3 3 1 0 3 2 2 0 1\n",
      " 1 0 1 2 3 1 1 1 3 0 1 2 1 1 3 1 0 2 3 0 1 2 0 0 3 0] <--Labels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for data, labels in check_data:\n",
    "    print(data.shape, labels.shape)\n",
    "    print(labels, \"<--Labels\")\n",
    "    print()\n",
    "    num = num + 1\n",
    "    if num > 5: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2de71ca-839a-44a0-b6c9-15e445129640",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 150\n",
    "dataset = tf.data.Dataset.from_generator(tf_data_generator,args= [files, batch_size],output_types = (tf.float32, tf.float32),\n",
    "                                                output_shapes = ((None,400,62,1),(None,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98b5840a-0083-413e-8dac-5ae12b4e5628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 400, 62, 1) (150,)\n",
      "tf.Tensor(\n",
      "[2. 0. 1. 3. 1. 2. 1. 1. 1. 3. 2. 2. 0. 3. 1. 2. 1. 2. 2. 2. 0. 0. 1. 2.\n",
      " 0. 0. 1. 0. 3. 3. 2. 3. 3. 0. 2. 0. 0. 1. 2. 1. 1. 0. 0. 1. 2. 1. 0. 1.\n",
      " 1. 1. 2. 3. 2. 0. 1. 2. 0. 0. 2. 2. 3. 1. 2. 2. 1. 3. 2. 2. 3. 1. 0. 0.\n",
      " 2. 2. 1. 3. 0. 3. 3. 0. 1. 3. 0. 3. 2. 2. 2. 2. 0. 0. 0. 0. 1. 2. 1. 2.\n",
      " 2. 0. 0. 1. 0. 0. 3. 3. 3. 1. 3. 1. 3. 0. 3. 3. 3. 0. 1. 1. 0. 2. 1. 2.\n",
      " 2. 3. 3. 1. 3. 3. 1. 3. 2. 1. 3. 0. 2. 2. 2. 2. 1. 1. 3. 1. 2. 3. 0. 1.\n",
      " 2. 0. 1. 1. 1. 2.], shape=(150,), dtype=float32)\n",
      "\n",
      "(150, 400, 62, 1) (150,)\n",
      "tf.Tensor(\n",
      "[1. 2. 1. 3. 3. 3. 3. 3. 3. 3. 2. 2. 2. 1. 1. 3. 3. 1. 0. 3. 2. 2. 0. 1.\n",
      " 1. 0. 1. 2. 3. 1. 1. 1. 3. 0. 1. 2. 1. 1. 3. 1. 0. 2. 3. 0. 1. 2. 0. 0.\n",
      " 3. 0. 1. 3. 1. 3. 0. 3. 3. 0. 1. 3. 2. 3. 0. 2. 0. 3. 0. 2. 1. 2. 3. 2.\n",
      " 3. 2. 1. 0. 2. 3. 0. 0. 0. 3. 2. 0. 1. 0. 1. 3. 0. 0. 2. 2. 2. 0. 3. 2.\n",
      " 3. 2. 2. 0. 2. 0. 0. 2. 1. 1. 1. 3. 0. 2. 3. 1. 0. 1. 3. 1. 3. 0. 2. 0.\n",
      " 1. 3. 3. 0. 1. 0. 0. 1. 2. 0. 2. 1. 1. 1. 2. 1. 3. 3. 3. 3. 2. 3. 1. 3.\n",
      " 2. 0. 3. 1. 0. 0.], shape=(150,), dtype=float32)\n",
      "\n",
      "(60, 400, 62, 1) (60,)\n",
      "tf.Tensor(\n",
      "[1. 3. 2. 0. 1. 3. 1. 0. 3. 0. 3. 0. 0. 2. 0. 0. 2. 2. 2. 1. 2. 2. 2. 1.\n",
      " 1. 2. 0. 3. 2. 3. 2. 2. 2. 0. 0. 2. 3. 0. 3. 1. 1. 0. 2. 3. 3. 0. 3. 3.\n",
      " 0. 3. 3. 0. 0. 0. 3. 1. 1. 1. 0. 3.], shape=(60,), dtype=float32)\n",
      "\n",
      "(150, 400, 62, 1) (150,)\n",
      "tf.Tensor(\n",
      "[1. 3. 3. 3. 1. 2. 2. 1. 1. 0. 2. 2. 3. 0. 2. 0. 3. 0. 3. 2. 0. 1. 0. 3.\n",
      " 1. 0. 0. 0. 1. 2. 3. 2. 2. 0. 3. 1. 0. 1. 1. 2. 3. 3. 0. 3. 0. 3. 1. 3.\n",
      " 1. 1. 1. 3. 0. 0. 1. 3. 0. 0. 2. 3. 3. 3. 2. 3. 1. 0. 3. 2. 0. 1. 3. 0.\n",
      " 1. 2. 3. 0. 3. 3. 3. 3. 3. 0. 3. 2. 1. 2. 0. 1. 2. 3. 1. 0. 2. 2. 3. 0.\n",
      " 2. 3. 1. 1. 2. 2. 1. 2. 1. 0. 2. 2. 3. 1. 3. 3. 0. 2. 0. 2. 1. 1. 1. 3.\n",
      " 0. 2. 3. 0. 2. 0. 1. 3. 2. 1. 0. 0. 1. 0. 2. 3. 2. 0. 1. 0. 0. 0. 3. 1.\n",
      " 1. 3. 1. 1. 0. 2.], shape=(150,), dtype=float32)\n",
      "\n",
      "(150, 400, 62, 1) (150,)\n",
      "tf.Tensor(\n",
      "[3. 0. 0. 2. 2. 3. 1. 2. 3. 1. 0. 0. 3. 3. 0. 2. 1. 1. 3. 0. 2. 1. 3. 2.\n",
      " 0. 1. 3. 2. 3. 2. 2. 3. 2. 1. 2. 1. 2. 2. 2. 2. 1. 1. 0. 3. 0. 1. 1. 2.\n",
      " 1. 1. 0. 3. 0. 0. 0. 0. 3. 3. 1. 3. 0. 2. 2. 1. 1. 0. 1. 3. 0. 2. 2. 0.\n",
      " 2. 0. 1. 2. 1. 2. 2. 3. 1. 1. 2. 1. 3. 0. 2. 3. 2. 3. 2. 3. 1. 1. 2. 2.\n",
      " 1. 1. 2. 2. 3. 2. 3. 3. 3. 0. 3. 2. 3. 0. 0. 1. 2. 0. 0. 1. 0. 0. 0. 0.\n",
      " 2. 3. 2. 0. 0. 3. 1. 0. 3. 2. 1. 1. 2. 1. 0. 2. 0. 3. 1. 2. 0. 0. 1. 2.\n",
      " 1. 1. 0. 3. 0. 3.], shape=(150,), dtype=float32)\n",
      "\n",
      "(60, 400, 62, 1) (60,)\n",
      "tf.Tensor(\n",
      "[3. 0. 2. 0. 3. 3. 3. 3. 0. 1. 3. 1. 3. 0. 2. 0. 2. 2. 1. 0. 3. 1. 3. 1.\n",
      " 2. 2. 2. 3. 1. 0. 3. 1. 1. 3. 2. 0. 3. 2. 0. 2. 1. 0. 1. 2. 2. 0. 1. 1.\n",
      " 2. 3. 1. 3. 2. 1. 3. 0. 1. 1. 0. 2.], shape=(60,), dtype=float32)\n",
      "\n",
      "(150, 400, 62, 1) (150,)\n",
      "tf.Tensor(\n",
      "[2. 3. 1. 2. 3. 1. 3. 3. 1. 1. 2. 1. 0. 2. 1. 3. 2. 0. 1. 1. 3. 0. 0. 0.\n",
      " 3. 2. 2. 3. 3. 1. 0. 2. 3. 3. 3. 0. 3. 3. 0. 2. 2. 3. 0. 1. 0. 0. 2. 0.\n",
      " 1. 1. 1. 3. 0. 3. 0. 2. 2. 2. 3. 0. 0. 1. 3. 2. 2. 2. 1. 2. 1. 0. 3. 1.\n",
      " 0. 0. 3. 2. 2. 0. 1. 3. 1. 1. 0. 0. 0. 3. 0. 3. 3. 1. 1. 2. 0. 0. 0. 2.\n",
      " 1. 2. 2. 0. 1. 2. 1. 1. 3. 3. 2. 0. 0. 0. 1. 0. 2. 2. 2. 2. 1. 2. 1. 3.\n",
      " 2. 0. 3. 2. 3. 2. 0. 1. 3. 1. 0. 3. 3. 0. 3. 1. 0. 1. 1. 1. 2. 0. 0. 3.\n",
      " 2. 3. 2. 0. 1. 3.], shape=(150,), dtype=float32)\n",
      "\n",
      "(150, 400, 62, 1) (150,)\n",
      "tf.Tensor(\n",
      "[3. 2. 0. 3. 2. 0. 0. 2. 1. 3. 3. 3. 1. 3. 2. 0. 2. 1. 3. 2. 1. 0. 1. 2.\n",
      " 0. 0. 3. 1. 2. 0. 2. 1. 2. 2. 0. 2. 1. 2. 0. 0. 3. 2. 2. 1. 1. 0. 1. 0.\n",
      " 3. 2. 3. 2. 3. 2. 0. 2. 2. 0. 3. 0. 2. 3. 3. 3. 1. 2. 0. 3. 1. 3. 2. 2.\n",
      " 2. 3. 1. 1. 3. 0. 2. 0. 3. 1. 2. 3. 3. 1. 0. 0. 2. 1. 1. 0. 0. 2. 3. 0.\n",
      " 3. 3. 1. 1. 2. 3. 1. 1. 0. 2. 2. 2. 3. 3. 1. 1. 3. 1. 0. 1. 2. 1. 2. 1.\n",
      " 2. 3. 1. 1. 0. 3. 2. 3. 1. 1. 3. 1. 2. 3. 0. 2. 3. 0. 0. 3. 2. 1. 1. 2.\n",
      " 0. 0. 0. 3. 0. 2.], shape=(150,), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check whether dataset works or not.\n",
    "num = 0\n",
    "for data, labels in dataset:\n",
    "    print(data.shape, labels.shape)\n",
    "    print(labels)\n",
    "    print()\n",
    "    num = num + 1\n",
    "    if num > 7: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "105ac942-89ee-459e-b787-f1983b93bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building data pipeline and training CNN modelÂ¶\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2a2d5-d22c-4496-bde1-0a2d6d24f1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7be4c349-0fac-47bd-a9de-9f36590defe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Neutral_files = glob.glob(\"./EEG FOLDER/Neutral/*\")\n",
    "Sad_files = glob.glob(\"./EEG FOLDER/Sad/*\")\n",
    "Fear_files = glob.glob(\"./EEG FOLDER/Fear/*\")\n",
    "Happy_files = glob.glob(\"./EEG FOLDER/Happy/*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b181c05e-6a95-4be6-9dfc-a3e0ef56c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2f84d8e-ef55-487f-88ef-25df244da48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Neutral_train, Neutral_test = train_test_split(Neutral_files, test_size = 20, random_state = 5)\n",
    "Sad_train, Sad_test = train_test_split(Sad_files, test_size = 20, random_state = 54)\n",
    "Fear_train, Fear_test = train_test_split(Fear_files, test_size = 20, random_state = 543)\n",
    "Happy_train, Happy_test = train_test_split(Happy_files, test_size = 20, random_state = 5432)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a26ca310-c8b8-400c-906b-175ddb5564f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Neutral_train, Neutral_val = train_test_split(Neutral_train, test_size = 10, random_state = 1)\n",
    "Sad_train, Sad_val = train_test_split(Sad_train, test_size = 10, random_state = 12)\n",
    "Fear_train, Fear_val = train_test_split(Fear_train, test_size = 10, random_state = 123)\n",
    "Happy_train, Happy_val = train_test_split(Happy_train, test_size = 10, random_state = 1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86759485-b300-4130-93f9-c9f9f9c12cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_names = Neutral_train + Sad_train + Fear_train + Happy_train \n",
    "validation_file_names = Neutral_val + Sad_val + Fear_val + Happy_val\n",
    "test_file_names = Neutral_test + Sad_test + Fear_test + Happy_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c7c619b-3e1e-4770-81d0-484e0d36fff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train_files: 240\n",
      "Number of validation_files: 40\n",
      "Number of test_files: 80\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train_files:\" ,len(train_file_names))\n",
    "print(\"Number of validation_files:\" ,len(validation_file_names))\n",
    "print(\"Number of test_files:\" ,len(test_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37885d64-1be7-4da5-b265-ca8acf95d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [train_file_names, batch_size], \n",
    "                                              output_shapes = ((None,400,62,1),(None,)),\n",
    "                                              output_types = (tf.float32, tf.float32))\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [validation_file_names, batch_size],\n",
    "                                                   output_shapes = ((None,400,62,1),(None,)),\n",
    "                                                   output_types = (tf.float32, tf.float32))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [test_file_names, batch_size],\n",
    "                                             output_shapes = ((None,400,62,1),(None,)),\n",
    "                                             output_types = (tf.float32, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d13e9b3f-d44b-40d9-905f-fda0ae258019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the model.\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04445e96-6ee5-49f3-9b67-0dc696ae45bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 398, 60, 16)       160       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 199, 30, 16)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 197, 28, 62)       8990      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 98, 14, 62)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 85064)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                1361040   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,370,275\n",
      "Trainable params: 1,370,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(16, 3, activation = \"relu\", input_shape = (400,62,1)),\n",
    "    layers.MaxPool2D(2),\n",
    "    layers.Conv2D(62, 3, activation = \"relu\"),\n",
    "    layers.MaxPool2D(2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(16, activation = \"relu\"),\n",
    "    layers.Dense(5, activation = \"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31c7fd16-4150-4d42-889b-c642a6b07414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model.\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ae9fd13-3dee-4242-8936-13407fac868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_per_epoch =  3\n",
      "validation_steps =  1\n",
      "steps =  1\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = int(np.ceil(len(train_file_names)/batch_size))\n",
    "validation_steps = int(np.ceil(len(validation_file_names)/batch_size))\n",
    "steps = int(np.ceil(len(test_file_names)/batch_size))\n",
    "print(\"steps_per_epoch = \", steps_per_epoch)\n",
    "print(\"validation_steps = \", validation_steps)\n",
    "print(\"steps = \", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b892b76-ab77-4e67-8b87-59d8f5343a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/3 [=========>....................] - ETA: 5s - loss: 1.6026 - accuracy: 0.4000"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,\n",
    "         validation_steps = validation_steps, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c6d835cb-c50c-4fed-a627-1da2513894a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 3s 271ms/step - loss: 939.6616 - accuracy: 0.2700\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset, steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1253738-fa3f-4835-95d4-2a9e3d84a2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  939.6615600585938\n",
      "Test accuracy: 0.27000001072883606\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: \", test_loss)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb7c10-1c85-40b9-8dc0-9fa3cb789daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
