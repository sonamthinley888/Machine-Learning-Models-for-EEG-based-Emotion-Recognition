{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b91aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for creating validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv1d, Conv2d,MaxPool1d, MaxPool2d, Module, Softmax, BatchNorm1d, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# torchvision for pre-trained models\n",
    "from torchvision import models\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5dee07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_csv = pd.read_csv(r\"C:\\Users\\sonam\\University of Canberra\\4th Semester\\Capstone\\Models\\Session_1_labels.csv\")\n",
    "master_csv.head()\n",
    "master_csv.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d84ba3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 360/360 [03:20<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 1, 6002, 62)\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd() \n",
    "folder_path = os.path.join(cwd,\"session_1_all_200\")\n",
    "\n",
    "# define custom transform function\n",
    "transform_tensor = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "eeg_data = []\n",
    "for filename in tqdm(master_csv['filename']):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    file = pd.read_csv(file_path)\n",
    "    \n",
    "    # Drop rows below row 6002\n",
    "    file = file.iloc[:6002]\n",
    "    \n",
    "    file = file.astype('float32')\n",
    "    eeg_values = file.drop(columns=['time']).values  # Assuming 'time' column is not needed\n",
    "    \n",
    "    eeg_np = np.array(eeg_values)\n",
    "    eeg_tr = transform_tensor(eeg_np)\n",
    "    \n",
    "    # Calculate mean and std\n",
    "    mean, std = eeg_tr.mean(), eeg_tr.std()\n",
    "    \n",
    "    # Define normalization transform\n",
    "    transform_norm = transforms.Compose([\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "    \n",
    "    # Apply normalization\n",
    "    eeg_normalized = transform_norm(eeg_tr)\n",
    "    \n",
    "    eeg_data.append(eeg_normalized)\n",
    "\n",
    "# Convert the list of normalized tensors to a NumPy array\n",
    "eeg_numpy = np.array([item.numpy() for item in eeg_data])\n",
    "print(eeg_numpy.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de582e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eeg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8fca2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 1.8447617e-01,  5.6057847e-01,  5.3978020e-01, ...,\n",
       "           1.4445057e+00,  1.6403562e+00,  2.2660379e+00],\n",
       "         [ 1.0909349e+00,  1.1723949e+00,  9.6441185e-01, ...,\n",
       "           1.2330564e+00,  1.3526466e+00,  1.9609965e+00],\n",
       "         [ 7.8935969e-01,  7.8416008e-01,  7.6682818e-01, ...,\n",
       "           1.4705036e+00,  1.5588962e+00,  2.3405654e+00],\n",
       "         ...,\n",
       "         [-6.6825360e-01, -6.4918852e-01, -1.6665713e+00, ...,\n",
       "          -1.1760784e+00, -2.2802319e-01, -7.0985019e-01],\n",
       "         [-4.2560688e-01, -6.5438807e-01, -1.8052266e+00, ...,\n",
       "          -1.4793868e+00, -8.1904113e-01, -1.3164668e+00],\n",
       "         [-7.8611052e-01, -1.0270240e+00, -2.0305417e+00, ...,\n",
       "          -1.0148917e+00, -2.4362190e-01, -6.8905187e-01]]],\n",
       "\n",
       "\n",
       "       [[[-3.3001888e-01, -2.3145182e-01,  3.5995033e-01, ...,\n",
       "           1.3116320e+00,  1.5189626e+00,  4.7721109e-01],\n",
       "         [-1.6857287e-01,  1.5431912e-01,  5.3499174e-01, ...,\n",
       "           1.2742445e+00,  1.5104656e+00,  5.6728095e-01],\n",
       "         [-7.1705274e-02,  2.0064030e-02,  6.7648113e-02, ...,\n",
       "           1.2759440e+00,  1.3983030e+00,  3.6334917e-01],\n",
       "         ...,\n",
       "         [-1.4057590e+00, -1.3972619e+00,  1.1501859e+00, ...,\n",
       "          -1.0624738e+00, -1.5179214e+00, -1.6045924e+00],\n",
       "         [-1.4431466e+00, -1.0199879e+00,  7.8310877e-01, ...,\n",
       "          -9.5031142e-01, -1.3955624e+00, -1.5417135e+00],\n",
       "         [-1.6929629e+00, -1.3938630e+00,  1.0533184e+00, ...,\n",
       "          -8.1945515e-01, -1.2664056e+00, -1.4278516e+00]]],\n",
       "\n",
       "\n",
       "       [[[-1.4951001e-01, -6.6316384e-01, -8.8063830e-01, ...,\n",
       "           1.8429700e+00,  1.9051055e+00,  2.4021900e+00],\n",
       "         [-2.0750318e-01, -9.4691616e-01, -1.4253601e+00, ...,\n",
       "           1.8781800e+00,  1.7228411e+00,  2.5098915e+00],\n",
       "         [-7.8536379e-01, -1.7691766e+00, -2.2704036e+00, ...,\n",
       "           1.7497666e+00,  1.6317091e+00,  2.3214138e+00],\n",
       "         ...,\n",
       "         [-1.0815432e+00, -4.9332672e-01, -9.7798395e-01, ...,\n",
       "          -9.6762800e-01, -8.7028235e-01,  3.8967833e-02],\n",
       "         [-1.0297636e+00, -4.2083523e-01, -8.8892299e-01, ...,\n",
       "          -6.9837397e-01, -6.7144859e-01,  5.5537313e-02],\n",
       "         [-2.0543198e-01,  3.8485575e-01,  1.6865060e-03, ...,\n",
       "          -5.6167579e-01, -4.4568947e-01,  3.4964558e-01]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 1.0023944e+00,  8.2835972e-01,  6.9967949e-01, ...,\n",
       "          -1.4201576e-01, -7.1158397e-01, -6.6728425e-01],\n",
       "         [ 1.4137492e+00,  1.3135474e+00,  1.0920486e+00, ...,\n",
       "          -1.4834429e-01, -5.3860402e-01, -5.0168753e-01],\n",
       "         [ 1.1521697e+00,  9.9712068e-01,  8.8637131e-01, ...,\n",
       "          -5.6580532e-02, -4.4578546e-01, -4.5422351e-01],\n",
       "         ...,\n",
       "         [ 2.1921592e+00,  1.4875822e+00,  6.7014635e-01, ...,\n",
       "           2.9676270e-01,  1.8601334e-01,  5.3114086e-02],\n",
       "         [-4.4894972e-01, -1.1936073e+00, -1.6366049e+00, ...,\n",
       "          -1.7041092e+00, -2.2694583e+00, -2.2589109e+00],\n",
       "         [ 1.6911501e+00,  1.4506657e+00,  8.3468819e-01, ...,\n",
       "           2.3875113e-01, -1.6499808e-02, -8.1894673e-02]]],\n",
       "\n",
       "\n",
       "       [[[-2.2755017e+00, -1.7690346e+00, -1.2370943e+00, ...,\n",
       "           2.3585613e-01,  2.6432618e-01,  1.9989398e-01],\n",
       "         [-3.4397771e+00, -3.2314963e+00, -2.5062592e+00, ...,\n",
       "          -4.8488566e-01, -6.1375010e-01, -5.7179421e-01],\n",
       "         [-2.3414323e+00, -1.9818108e+00, -1.4019209e+00, ...,\n",
       "          -8.3308101e-02, -8.1809677e-02, -1.3275607e-01],\n",
       "         ...,\n",
       "         [-1.6206908e+00, -1.6806277e+00, -1.7720315e+00, ...,\n",
       "          -1.0677724e+00, -1.1591763e+00, -1.1262110e+00],\n",
       "         [-1.8499495e+00, -1.8079935e+00, -1.8214794e+00, ...,\n",
       "          -4.9687305e-01, -3.9048499e-01, -3.4253541e-01],\n",
       "         [-1.4124099e+00, -1.3929304e+00, -1.5787349e+00, ...,\n",
       "          -1.2026306e+00, -1.0033402e+00, -1.0228198e+00]]],\n",
       "\n",
       "\n",
       "       [[[-1.1959627e+00, -1.3593702e+00, -1.1788902e+00, ...,\n",
       "          -3.1307396e-01, -7.1671504e-01, -7.3866528e-01],\n",
       "         [-1.7105747e+00, -1.8117898e+00, -1.7361832e+00, ...,\n",
       "          -8.7036699e-01, -1.2898611e+00, -1.2862027e+00],\n",
       "         [-1.3922956e+00, -1.3398589e+00, -1.2142545e+00, ...,\n",
       "          -6.1671942e-01, -9.4841236e-01, -9.5938754e-01],\n",
       "         ...,\n",
       "         [-1.2649667e-01, -3.0697668e-01, -3.3014640e-01, ...,\n",
       "           8.2030907e-02, -2.5941774e-01, -2.3015071e-01],\n",
       "         [-1.2362049e+00, -1.4410741e+00, -1.2557162e+00, ...,\n",
       "          -6.1793888e-01, -9.0817022e-01, -8.7280589e-01],\n",
       "         [ 2.5153577e-01,  8.8128209e-02,  2.7155226e-02, ...,\n",
       "          -3.1429344e-01, -4.3867832e-01, -4.5818967e-01]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45a177b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 62, 6002)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d298ede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 0 2 0 0 1 0 1 2 1 1 1 2 3 2 2 3 3 0 3 0 3 1 2 3 0 2 0 0 1 0 1 2 1 1\n",
      " 1 2 3 2 2 3 3 0 3 0 3]\n"
     ]
    }
   ],
   "source": [
    "# defining the target\n",
    "label_emotion = master_csv['emotion'].values\n",
    "print(label_emotion[:48])  # Display the first 10 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8ed5147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((270, 62, 6002), (270,)), ((90, 62, 6002), (90,)))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "train_x, val_x, train_y, val_y = train_test_split(eeg_numpy, label_emotion, test_size = 0.25, random_state = 13, stratify=label_emotion)\n",
    "(train_x.shape, train_y.shape), (val_x.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d56606ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting training images into torch format\n",
    "# Reshape training data\n",
    "\n",
    "train_x = torch.from_numpy(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "081b3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the target into torch format\n",
    "train_y = train_y.astype(int)\n",
    "train_y = torch.from_numpy(train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ea0f80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([270, 62, 6002]), torch.Size([270]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of training data\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f9ee06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape test data\n",
    "\n",
    "val_x = torch.from_numpy(val_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "val_y = val_y.astype(int)\n",
    "val_y = torch.from_numpy(val_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06d69a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([90, 62, 6002]), torch.Size([90]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of validation data\n",
    "val_x.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92bedaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):   \n",
    "    def __init__(self, in_channels=62, num_classes=4):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(256, 512, kernel_size=3, stride=2, padding=1),  \n",
    "            nn.BatchNorm1d(512),                                      \n",
    "            nn.ReLU(inplace=True),                                    \n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)                     \n",
    "        )\n",
    "        \n",
    "        # Calculate the number of features after convolutional layers\n",
    "        self.num_features = self._calculate_num_features((62, 6000))  # Assuming input size is (62, 2000)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(self.num_features, 2048),  # Increased neurons\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 1024),                # Increased neurons\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "    \n",
    "    def _calculate_num_features(self, input_dim):\n",
    "        # Forward pass to get the shape after convolutional layers\n",
    "        input_tensor = torch.zeros(1, *input_dim)\n",
    "        conv_output = self.cnn_layers(input_tensor)\n",
    "        num_features = conv_output.view(1, -1).size(1)\n",
    "        return num_features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a2e923a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv1d(62, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=11776, out_features=2048, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=1024, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Net()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# batch size of the model\n",
    "batch_size = 200\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e470b91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 1 \t training loss: \t 1.3872495889663696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 2 \t training loss: \t 1.355642557144165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 3 \t training loss: \t 1.3294575810432434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 4 \t training loss: \t 1.3134034276008606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 5 \t training loss: \t 1.2839776277542114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 6 \t training loss: \t 1.2878819704055786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 7 \t training loss: \t 1.2356016635894775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 8 \t training loss: \t 1.2219054698944092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 9 \t training loss: \t 1.2216623425483704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 10 \t training loss: \t 1.1848611235618591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 11 \t training loss: \t 1.1707348227500916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 12 \t training loss: \t 1.1687666177749634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 13 \t training loss: \t 1.1410840153694153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 14 \t training loss: \t 1.1532792448997498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 15 \t training loss: \t 1.1515308022499084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 16 \t training loss: \t 1.1373720169067383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 17 \t training loss: \t 1.1639060378074646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 18 \t training loss: \t 1.1094833612442017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 19 \t training loss: \t 1.127306580543518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t 20 \t training loss: \t 1.1192717552185059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "        \n",
    "    permutation = torch.randperm(train_x.size()[0])\n",
    "\n",
    "    training_loss = []\n",
    "    for i in tqdm(range(0,train_x.size()[0], batch_size)):\n",
    "\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = train_x[indices], train_y[indices]\n",
    "        \n",
    "        # Convert batch_y to Long data type\n",
    "        batch_y = batch_y.long()  # Convert to Long data type\n",
    "        \n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # in case you wanted a semi-full example\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        training_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    training_loss = np.average(training_loss)\n",
    "    print('epoch: \\t', epoch, '\\t training loss: \\t', training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "27b031c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: \t 0.5857142857142856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction for training set\n",
    "prediction = []\n",
    "target = []\n",
    "permutation = torch.randperm(train_x.size()[0])\n",
    "for i in tqdm(range(0,train_x.size()[0], batch_size)):\n",
    "    indices = permutation[i:i+batch_size]\n",
    "    batch_x, batch_y = train_x[indices], train_y[indices]\n",
    "\n",
    "    #if torch.cuda.is_available():\n",
    "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(batch_x)\n",
    "\n",
    "    softmax = torch.exp(output).cpu()\n",
    "    prob = list(softmax.numpy())\n",
    "    predictions = np.argmax(prob, axis=1)\n",
    "    prediction.append(predictions)\n",
    "    target.append(batch_y)\n",
    "    \n",
    "# training accuracy\n",
    "accuracy = []\n",
    "for i in range(len(prediction)):\n",
    "    accuracy.append(accuracy_score(target[i],prediction[i]))\n",
    "    \n",
    "print('training accuracy: \\t', np.average(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb708a18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: \t 0.3888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction for validation set\n",
    "prediction_val = []\n",
    "target_val = []\n",
    "permutation = torch.randperm(val_x.size()[0])\n",
    "for i in tqdm(range(0,val_x.size()[0], batch_size)):\n",
    "    indices = permutation[i:i+batch_size]\n",
    "    batch_x, batch_y = val_x[indices], val_y[indices]\n",
    "\n",
    "    #if torch.cuda.is_available():\n",
    "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(batch_x)\n",
    "\n",
    "    softmax = torch.exp(output).cpu()\n",
    "    prob = list(softmax.numpy())\n",
    "    predictions = np.argmax(prob, axis=1)\n",
    "    prediction_val.append(predictions)\n",
    "    target_val.append(batch_y)\n",
    "    \n",
    "# validation accuracy\n",
    "accuracy_val = []\n",
    "for i in range(len(prediction_val)):\n",
    "    accuracy_val.append(accuracy_score(target_val[i],prediction_val[i]))\n",
    "    \n",
    "print('validation accuracy: \\t', np.average(accuracy_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb8885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a0069c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
